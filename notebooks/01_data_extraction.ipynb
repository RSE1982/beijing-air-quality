{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# üî¨ Beijing Air Quality\n",
        "## üìò Notebook 01 ‚Äì Data Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Field         | Description                                        |\n",
        "|:--------------|:---------------------------------------------------|\n",
        "| Author:       |\tRobert Steven Elliott                            |\n",
        "| Course:       |\tCode Institute ‚Äì Data Analytics with AI Bootcamp |\n",
        "| Project Type: |\tCapstone                                         |\n",
        "| Date:         |\tDecember 2025                                    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "- Load all 12 raw Beijing air-quality CSV files.\n",
        "- Verify file structure, column consistency, and encoding.\n",
        "- Concatenate the raw station datasets into a single combined dataframe.\n",
        "- Save the combined dataset in the `data/combined/` directory for downstream cleaning and feature engineering.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- Raw dataset folder: data/raw/\n",
        "    - Contains 12 station CSV files (e.g., aotizhongxin.csv, changping.csv, etc.)\n",
        "- Expected columns (as defined in metadata):\n",
        "    - PM2.5, PM10, SO2, NO2, CO, O3, TEMP, PRES, DEWP, RAIN, wd, WSPM, station, year, month, day, hour\n",
        "\n",
        "## Outputs\n",
        "\n",
        "- A single concatenated dataframe containing all raw station data.\n",
        "- Exported combined CSV: data/combined/combined_stations.csv\n",
        "- Basic shape summary and verification logs\n",
        "\n",
        "## Additional Comments\n",
        "- This notebook performs no cleaning‚Äîit preserves raw data to ensure full provenance.\n",
        "- Any inconsistencies identified here (missing timestamps, unexpected dtypes, column mismatches) will be addressed in Notebook 02 ‚Äì Data Cleaning.\n",
        "\n",
        "## Citation  \n",
        "This project uses data from:\n",
        "\n",
        "**Chen, Song (2017). _Beijing Multi-Site Air Quality_. UCI Machine Learning Repository.**  \n",
        "DOI: https://doi.org/10.24432/C5RK5G  \n",
        "Mirrored on Kaggle by Manu Siddhartha (CC BY 4.0 Licence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Overview\n",
        "\n",
        "This notebook performs the first step of the ETL pipeline: **extracting** the raw Beijing air-quality data.\n",
        "The goal is to load all 12 raw station CSV files, verify their structure, and combine them into a single\n",
        "dataset that can be used for cleaning and preprocessing in later notebooks.\n",
        "\n",
        "This step focuses only on *data extraction*, not data cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries\n",
        "\n",
        "In this section we import all necessary Python libraries:\n",
        "\n",
        "- `pathlib` ‚Äì handles directory paths in a platform-independent way  \n",
        "- `os` ‚Äì used to extract file names  \n",
        "- `glob` ‚Äì used to search for all CSV files in the raw dataset folder  \n",
        "- `pandas` ‚Äì the main library for loading and manipulating dataframes  \n",
        "\n",
        "These tools together allow us to dynamically load each raw station file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path # Path is needed to handle file paths\n",
        "import os # os is needed for operating system dependent functionality\n",
        "import glob # glob is needed to find files matching a pattern\n",
        "import pandas as pd # pandas is needed for data manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up Project Paths\n",
        "\n",
        "We define the project root and confirm that the notebook is pointing to the correct folder.\n",
        "\n",
        "This ensures the notebook remains portable, even if the directory structure changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root : /home/robert/Projects/beijing-air-quality\n",
            "Current working directory : /home/robert/Projects/beijing-air-quality/notebooks\n"
          ]
        }
      ],
      "source": [
        "project_root = Path.cwd().parent # Assuming this script is in the 'notebooks' directory\n",
        "print(f\"Project root : {project_root}\") # Print the project root directory\n",
        "\n",
        "current_dir = project_root / \"notebooks\" # Set current working directory to notebooks\n",
        "print(f\"Current working directory : {current_dir}\") # Print the current working directory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load Raw Station CSV Files\n",
        "\n",
        "The raw dataset contains **12 separate files**, one for each monitoring station:\n",
        "\n",
        "- We use `glob` to find all `.csv` files in `data/raw/`.\n",
        "- Each file is read using `pandas.read_csv()`.\n",
        "- A new column `station` is added based on the file name to preserve provenance.\n",
        "- All station dataframes are collected into a list for later combination.\n",
        "\n",
        "This maintains traceability and aligns with the Capstone data-governance requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfs = [] # List to hold individual dataframes\n",
        "files = glob.glob(str(project_root / \"data\" / \"raw\" / \"*.csv\")) # Get all CSV files in the raw data directory\n",
        "\n",
        "for file in files:\n",
        "    df = pd.read_csv(file) # Read each CSV file\n",
        "    station = os.path.basename(file).split(\".\")[0] # Assuming filename format is like 'beijing_stationname_dates.csv'\n",
        "    df[\"station\"] = station # Add station column\n",
        "    dfs.append(df) # Collect all dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine All Station Files\n",
        "\n",
        "Once all 12 files are loaded individually, they are concatenated into a single dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "combined_df = pd.concat(dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the Combined Dataset\n",
        "\n",
        "The combined dataframe is saved to `data/combined/combined_stations.csv`.\n",
        "\n",
        "This file will be used in:\n",
        "\n",
        "- Notebook 02 ‚Äì Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined dataset shape: (420768, 18)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "combined_df.to_csv(project_root / \"data\" / \"combined\" / \"combined_stations.csv\", index=False) # Save the combined dataframe\n",
        "print(\"Combined dataset shape:\", combined_df.shape) # Print the shape of the combined dataset"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
