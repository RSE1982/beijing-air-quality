{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# üî¨ Beijing Air Quality\n",
        "## üìò Notebook 01 ‚Äì Data Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Field         | Description                                        |\n",
        "|:--------------|:---------------------------------------------------|\n",
        "| Author:       |\tRobert Steven Elliott                            |\n",
        "| Course:       |\tCode Institute ‚Äì Data Analytics with AI Bootcamp |\n",
        "| Project Type: |\tCapstone                                         |\n",
        "| Date:         |\tDecember 2025                                    |\n",
        "\n",
        "This project complies with the CC BY 4.0 licence by including proper attribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "- Load all 12 raw Beijing air-quality CSV files.\n",
        "- Verify file structure, column consistency, and encoding.\n",
        "- Concatenate the raw station datasets into a single combined dataframe.\n",
        "- Save the combined dataset in the `data/combined/` directory for downstream cleaning and feature engineering.\n",
        "\n",
        "## Inputs\n",
        "\n",
        "- Raw dataset folder: data/raw/\n",
        "    - Contains 12 station CSV files (e.g., aotizhongxin.csv, changping.csv, etc.)\n",
        "- Expected columns (as defined in metadata):\n",
        "    - PM2.5, PM10, SO2, NO2, CO, O3, TEMP, PRES, DEWP, RAIN, wd, WSPM, station, year, month, day, hour\n",
        "\n",
        "## Outputs\n",
        "\n",
        "- A single concatenated dataframe containing all raw station data.\n",
        "- Exported combined CSV: data/combined/combined_stations.csv\n",
        "- Basic shape summary and verification logs\n",
        "\n",
        "## Additional Comments\n",
        "- This notebook performs no cleaning‚Äîit preserves raw data to ensure full provenance.\n",
        "- Any inconsistencies identified here (missing timestamps, unexpected dtypes, column mismatches) will be addressed in Notebook 02 ‚Äì Data Cleaning.\n",
        "\n",
        "## Citation  \n",
        "This project uses data from:\n",
        "\n",
        "**Chen, Song (2017). _Beijing Multi-Site Air Quality_. UCI Machine Learning Repository.**  \n",
        "Chen, Song (2017). *Beijing Multi-Site Air Quality.*  \n",
        "UCI Machine Learning Repository ‚Äî Licensed under **CC BY 4.0**.  \n",
        "DOI: https://doi.org/10.24432/C5RK5G  \n",
        "Kaggle mirror by Manu Siddhartha."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Overview\n",
        "\n",
        "This notebook performs the first step of the ETL pipeline: **extracting** the raw Beijing air-quality data.\n",
        "The goal is to load all 12 raw station CSV files, verify their structure, and combine them into a single\n",
        "dataset that can be used for cleaning and preprocessing in later notebooks.\n",
        "\n",
        "This step focuses only on *data extraction*, not data cleaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries\n",
        "\n",
        "(The following libraries support analysis, plotting, and data manipulation.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path # Path is needed to handle file paths\n",
        "import os # os is needed for operating system dependent functionality\n",
        "import glob # glob is needed to find files matching a pattern\n",
        "import pandas as pd # pandas is needed for data manipulation\n",
        "import sys # sys is needed to manipulate Python runtime environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up Project Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root : /home/robert/Projects/beijing-air-quality\n"
          ]
        }
      ],
      "source": [
        "PROJECT_PATH = Path.cwd().parent # Assuming this script is in the 'notebooks' directory\n",
        "sys.path.append(str(PROJECT_PATH)) # Add project root to sys.path\n",
        "print(f\"Project root : {PROJECT_PATH}\") # Print the project root directory\n",
        "\n",
        "RAW_FOLDER = PROJECT_PATH / \"data\" / \"raw\" # Define raw data folder path\n",
        "COMBINED_FILEPATH = PROJECT_PATH / \"data\" / \"combined\" / \"beijing_combined.csv\" # Define combined data file path\n",
        "RAW_METADATA_PATH = PROJECT_PATH / \"data\" / \"raw\" / \"_metadata.yml\" # Define metadata file path\n",
        "COMBINED_METADATA_PATH = PROJECT_PATH / \"data\" / \"combined\" / \"_metadata.yml\" # Define combined metadata file path\n",
        "\n",
        "if not COMBINED_FILEPATH.parent.exists(): # Check if combined data directory exists\n",
        "    os.makedirs(COMBINED_FILEPATH.parent) # Create combined data directory if it doesn't exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create _metadata.yml for the raw files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Metadata written to: /home/robert/Projects/beijing-air-quality/data/raw/_metadata.yml\n"
          ]
        }
      ],
      "source": [
        "from utils.metadata_builder import MetadataBuilder\n",
        "\n",
        "# List all .csv files in the raw folder\n",
        "raw_files = sorted([f.name for f in RAW_FOLDER.glob(\"*.csv\")])\n",
        "\n",
        "builder = MetadataBuilder(\n",
        "    dataset_path=\"data/raw/\", \n",
        "    dataset_name=\"Beijing Multi-Site Air Quality ‚Äì Raw Stations\",\n",
        "    description=(\n",
        "        \"This folder contains the 12 raw CSV files from the Beijing Multi-Site Air Quality dataset. \"\n",
        "        \"These files are preserved exactly as provided by the source (Kaggle/UCI) with no cleaning, \"\n",
        "        \"renaming, or preprocessing applied. The raw data is kept intact to maintain full provenance.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add source + licence (same for all downstream datasets)\n",
        "builder.add_source_info()\n",
        "builder.add_licence()\n",
        "\n",
        "# Add file listing\n",
        "builder.add_file_list(raw_files)\n",
        "\n",
        "# Add column names (taken from one representative file)\n",
        "sample_df = pd.read_csv(RAW_FOLDER / raw_files[0], nrows=5)\n",
        "builder.add_columns(sample_df.columns)\n",
        "\n",
        "# Add notes\n",
        "builder.add_step(\"Downloaded the raw datasets from Kaggle / UCI repository\")\n",
        "builder.add_step(\"Stored raw files without modification\")\n",
        "builder.add_step(\"Verified encoding and column structure\")\n",
        "builder.add_step(\"Validated presence of all 12 station files\")\n",
        "\n",
        "builder.metadata[\"notes\"] = [\n",
        "    \"These files remain unmodified to maintain full provenance.\",\n",
        "    \"No cleaning, renaming, or type conversion steps were applied.\"\n",
        "]\n",
        "\n",
        "# Write metadata\n",
        "builder.write(RAW_METADATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialise metadata for Combined dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "builder = MetadataBuilder(\n",
        "    dataset_path=\"data/combined/combined_stations.csv\",\n",
        "    dataset_name=\"Beijing Air Quality ‚Äì Combined Dataset\",\n",
        "    description=\"A combined dataset merging all 12 raw station CSV files into one file.\"\n",
        ")\n",
        "\n",
        "builder.add_source_info()\n",
        "builder.add_licence()\n",
        "builder.add_creation_script(\"notebooks/01_data_extraction.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Load Raw Station CSV Files\n",
        "\n",
        "The raw dataset contains **12 separate files**, one for each monitoring station:\n",
        "\n",
        "- We use `glob` to find all `.csv` files in `data/raw/`.\n",
        "- Each file is read using `pandas.read_csv()`.\n",
        "- A new column `station` is added based on the file name to preserve provenance.\n",
        "- All station dataframes are collected into a list for later combination.\n",
        "\n",
        "This maintains traceability and aligns with the Capstone data-governance requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfs = [] # List to hold individual dataframes\n",
        "files = glob.glob(str(RAW_FOLDER / \"*.csv\")) # Get all CSV files in the raw data directory\n",
        "\n",
        "for file in files:\n",
        "    df = pd.read_csv(file) # Read each CSV file\n",
        "    station = os.path.basename(file).split(\".\")[0] # Assuming filename format is like 'beijing_stationname_dates.csv'\n",
        "    df[\"station\"] = station # Add station column\n",
        "    dfs.append(df) # Collect all dataframes\n",
        "\n",
        "builder.add_step(\"Loaded 12 raw station CSV files\") # Add step to metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combine All Station Files\n",
        "\n",
        "Once all 12 files are loaded individually, they are concatenated into a single dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "combined_df = pd.concat(dfs, ignore_index=True)\n",
        "builder.add_step(\"Concatenated all stations into a single dataframe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the Combined Dataset\n",
        "\n",
        "The combined dataframe is saved to `data/combined/combined_stations.csv`.\n",
        "\n",
        "This file will be used in:\n",
        "\n",
        "- Notebook 02 ‚Äì Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined dataset shape: (420768, 18)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "combined_df.to_csv(COMBINED_FILEPATH, index=False) # Save the combined dataframe\n",
        "print(\"Combined dataset shape:\", combined_df.shape) # Print the shape of the combined dataset\n",
        "builder.add_step(f\"Saved combined dataset to {COMBINED_FILEPATH}\") # Add step to metadata "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Metadata written to: /home/robert/Projects/beijing-air-quality/data/combined/_metadata.yml\n"
          ]
        }
      ],
      "source": [
        "builder.add_columns(df.columns) # Add columns the dataframe\n",
        "builder.add_record_count_from_df(combined_df) # Set record count from the combined dataframe    \n",
        "builder.add_record_stats(COMBINED_FILEPATH) # Add record statistics\n",
        "\n",
        "builder.write(COMBINED_METADATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### AI Assistance Note\n",
        "Some narrative text and minor formatting or wording improvements in this notebook were supported by AI-assisted tools (ChatGPT for documentation clarity, Copilot for small routine code suggestions, and Grammarly for proofreading). All analysis, code logic, feature engineering, modelling, and interpretations were independently created by the author."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
